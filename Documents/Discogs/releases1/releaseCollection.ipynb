{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# defining lists which will be used to parse the data\n",
    "\n",
    "important_keys = ['date_added', 'status', 'year', 'num_for_sale', 'artists', 'data_quality', 'country', 'resource_url', 'lowest_price', 'labels', 'formats', 'genres', 'community', 'date_changed']\n",
    "artists = ['resource_url', 'name']\n",
    "labels = ['resource_url', 'name'] # check if labels are often more than 1\n",
    "formats = ['name', 'descriptions']\n",
    "community = ['contributors', ['username'], 'rating', ['average'], ['count'], 'want', 'submitter', ['username'], 'have']\n",
    "l_l = ['date_added', 'status', 'year', 'num_for_sale', 'artists', 'data_quality', 'country', 'resource_url', 'lowest_price', 'labels', 'name_format', 'genres', 'username', 'average', 'count', 'want', 'submitter','have','date_changed']\n",
    "sub_keys = ['artists', 'labels', 'formats']\n",
    "\n",
    "# creating a dataframe to collect API calls that returned an error or no results\n",
    "\n",
    "dfmissing = pd.DataFrame(columns = ['address'])\n",
    "\n",
    "# creating a dictionary that will be used to fill in data from API calls that return an error or no results\n",
    "\n",
    "exc2 = {'date_added': np.nan, \n",
    "        'status': np.nan, \n",
    "        'year': np.nan, \n",
    "        'num_for_sale': np.nan, \n",
    "        'artists': [np.nan], \n",
    "        'data_quality': np.nan, \n",
    "        'country': np.nan, \n",
    "        'resource_url': np.nan, \n",
    "        'lowest_price': np.nan,\n",
    "        'labels': [np.nan], \n",
    "        'name_format': np.nan, \n",
    "        'genres': [np.nan], \n",
    "        'username': [np.nan], \n",
    "        'average': np.nan, \n",
    "        'count': np.nan, \n",
    "        'want': np.nan, \n",
    "        'submitter': np.nan,\n",
    "        'have': np.nan,\n",
    "        'date_changed': np.nan,\n",
    "       }\n",
    "\n",
    "# querying the csv file will username + contribution page # combinations. Should be updated based on userProfileCollection. \n",
    "\n",
    "dfpages = pd.DataFrame.from_csv('page_names.csv')\n",
    "\n",
    "df = pd.DataFrame(columns = l_l) # create dataframe to which all intermediate dataframes will be concatenated to. \n",
    "\n",
    "for n in range(40000, 60001): \n",
    "    check = time.time()\n",
    "    print(n, \" start\")\n",
    "    df4bis = pd.DataFrame(columns = l_l) # create dataframe that will be concatenated to df. \n",
    "    url = dfpages.loc[n,'address'] #get url address from dfpages dataframe. \n",
    "    headers = {\n",
    "                'Authorization': 'Discogs key=[key], secret=[secret]',\n",
    "            }\n",
    "    try:\n",
    "        r = requests.get(url, headers = headers)\n",
    "        store = json.loads(r.text) \n",
    "        print(\"-extraction-- %s seconds ---\" % (time.time() - check)) # get time of API call. \n",
    "        if 'contributions' in store.keys(): #parsing of the data.\n",
    "            for i in store['contributions']:\n",
    "                l_global = []\n",
    "                l_sub = []\n",
    "                for j in important_keys:\n",
    "                    if j in i.keys():\n",
    "                        if j in i.keys():\n",
    "                            if j == 'artists':\n",
    "                                for k in range(0,len(i[j])):\n",
    "                                    l1 = []\n",
    "                                    for l in artists:\n",
    "                                        l1.append({l: i[j][k][l]})\n",
    "                                l_global.append({j: l1})\n",
    "                            if j == 'labels':\n",
    "                                for k in range(0,len(i[j])):\n",
    "                                    l3 = []\n",
    "                                    for l in labels:\n",
    "                                        if l == 'resource_url':\n",
    "                                            l3.append({'resource_url_label': i[j][k][l]})\n",
    "                                        if l == 'name': \n",
    "                                            l3.append({'name_label': i[j][k][l]})\n",
    "                                l_global.append({j: l3})\n",
    "                            if j == 'formats':\n",
    "                                for k in range(0,len(i[j])):\n",
    "                                    for l in formats:\n",
    "                                        l4 = []\n",
    "                                        if l == 'name':\n",
    "                                            l4.append({'name_format': i[j][k][l]})\n",
    "                                l_global.append({j: l4})\n",
    "                            if j == 'community':\n",
    "                                for k in community:\n",
    "                                    l2 = []\n",
    "                                    if k == 'contributors':\n",
    "                                        for l in range(0, len(i[j][k])):                    \n",
    "                                            l2.append(i[j][k][l]['username'])\n",
    "                                        l_global.append({'username': l2})\n",
    "                                    if k == 'rating':\n",
    "                                        for l in i[j][k]:\n",
    "                                            l_global.append({l:i[j][k][l]})\n",
    "                                    if k == 'want':\n",
    "                                        l_global.append({k: i[j][k]})\n",
    "                                    if k == 'have':\n",
    "                                        l_global.append({k: i[j][k]})\n",
    "                                    if k == 'submitter':\n",
    "                                        try:\n",
    "                                            l_global.append({k: i[j][k]['username']})\n",
    "                                        except TypeError: \n",
    "                                            l_global.append({k: 'None'})\n",
    "                            elif j not in sub_keys : \n",
    "                                l_global.append({j: i[j]})\n",
    "                    elif j not in i.keys():\n",
    "                        l_global.append({j: ''})\n",
    "                for m in l_global:\n",
    "                    l_sub.append(list(m.values())[0])\n",
    "                df4bis.loc[len(df4bis)] = l_sub #adding parsed data to dataframe.\n",
    "        else:\n",
    "            dfmissing.loc[len(dfmissing)] = url #if error in API call, add the URL to a separate dataframe.\n",
    "            print(\"url missing.\")\n",
    "    except (requests.ConnectionError, ValueError): \n",
    "        dfmissing.loc[len(dfmissing)] = url #if error in API call, add the URL to a separate dataframe.\n",
    "        print('request timeout.')\n",
    "    df = pd.concat([df, df4bis]) #add parsed dataframe to bigger dataframe\n",
    "    print(\"-processing-- %s seconds ---\" % (time.time() - check))\n",
    "    if n%100==0:\n",
    "        df.to_csv('%s.csv' % n) #every 100 calls, create csv from dataframe. \n",
    "        df = pd.DataFrame(columns = l_l)  #reinitiate dataframe df. \n",
    "    total_time = (time.time() - check)\n",
    "    if total_time < 1: #if calls are too quick, add a pause time to the API calls (due to 60 calls per min. rate limitation).\n",
    "        time.sleep(1-total_time)\n",
    "        print(\"pause of\", 1 - total_time)\n",
    "dfmissing.to_csv('missing.csv') #create dataframe with missing URL adddesses. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
